# Omnivorous model architectures for Image and Video classfication and SSL 

This repository contains PyTorch pretrained models, inference examples for the following papers:
<details>
<summary>
  <a href="omnivore_README.md">Omnivore</a> A single vision model for many different visual modalities, CVPR 2022 [<b>bib</b>]
</summary>

```
@inproceedings{girdhar2022omnivore,
  title={{Omnivore: A Single Model for Many Visual Modalities}},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and van der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2022}
}
```
</details>
<details>
<summary>
<a href="omnimae_README.md">OmniMAE</a> Single Model Masked Pretraining on Images and Videos  [<b>bib</b>]
</summary>

```
@inproceedings{girdhar2022omnivore,
  title={{OmniMAE: Single Model Masked Pretraining on Images and Videos}},
  author={Girdhar, Rohit and El-Nouby Alaa and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={TODO},
  year={2022}
}
```
</details>

## Contributing
We welcome your pull requests! Please see [CONTRIBUTING](CONTRIBUTING.md) and [CODE_OF_CONDUCT](CODE_OF_CONDUCT.md) for more information.

## License
Omnivore is released under the CC-BY-NC 4.0 license. See [LICENSE](LICENSE) for additional details. However the Swin Transformer implementation is additionally licensed under the Apache 2.0 license (see [NOTICE](NOTICE) for additional details).

